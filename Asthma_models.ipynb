{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49407922-19db-442b-a2ce-1d055e916cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: (12736, 31)\n",
      "Train: (10188, 31), Test: (2548, 31)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: ASTHMA RISK CLASSIFICATION\n",
    "# =============================================================================\n",
    "# MISSING DATA HANDLING & TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"merged_nhanes_data_cleaned.csv\")\n",
    "\n",
    "# Drop rows with missing target variable\n",
    "df = df[df['Asthmatic'].notna()]\n",
    "\n",
    "# Identify weight columns\n",
    "weight_cols = [col for col in df.columns if 'weight' in col.lower() or col.startswith('WT')]\n",
    "\n",
    "# Drop rows with >40% missing data\n",
    "row_missing_pct = df.isnull().sum(axis=1) / len(df.columns)\n",
    "df = df[row_missing_pct <= 0.4]\n",
    "\n",
    "# Fill remaining missing values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['Asthmatic'] + weight_cols]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "print(f\"Final dataset: {df.shape}\")\n",
    "\n",
    "# Train/test split\n",
    "train, test = train_test_split(df, test_size=0.2, stratify=df['Asthmatic'], random_state=42)\n",
    "\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
    "\n",
    "# Save\n",
    "train.to_csv(\"train_cleaned.csv\", index=False)\n",
    "test.to_csv(\"test_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665c4b80-ea92-4813-88c8-1bfe10780443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interview weight: Full sample 2 year interview weight\n",
      "MEC weight: Full sample 2 year MEC exam weight\n",
      "Interview features: 19/19\n",
      "MEC features: 7/7\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SAMPLING WEIGHTS EXTRACTION\n",
    "# ==============================================================================\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"train_cleaned.csv\")\n",
    "test = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "# Define feature-weight mappings\n",
    "INTERVIEW_WEIGHT_FEATURES = [\n",
    "    \"Mother smoked when pregnant\", \"Child was overweight\",\n",
    "    \"Number of people who smoke inside the house\", \"Used any tobacco product in last 5 days\",\n",
    "    \"At least 100 cigarettes in life\", \"Age started smoking\", \"Currently smokes\",\n",
    "    \"Cigarettes smoked in entire life (12-17)\", \"Asthmatic\", \"Age when first had asthma\",\n",
    "    \"Still have asthma\", \"Asthma attack in past year\", \"Emergency care visit for asthma/past yr\",\n",
    "    \"Taking treatment for anemia\", \"Vigorous work activity\", \"High blood pressure\",\n",
    "    \"Shortness of breath on stairs\", \"Vigorous recreational activities\", \"Avg # alcoholic drinks/day\"\n",
    "]\n",
    "\n",
    "MEC_WEIGHT_FEATURES = [\n",
    "    \"BMI\", \"Eosinophils\", \"White blood cell count\", \"Basophils\",\n",
    "    \"Red blood cell count\", \"Total Cholesterol\", \"Urinary Total NNAL\"\n",
    "]\n",
    "\n",
    "# Find weight columns\n",
    "interview_weight = None\n",
    "mec_weight = None\n",
    "\n",
    "for col in train.columns:\n",
    "    if 'interview weight' in col.lower():\n",
    "        interview_weight = col\n",
    "    if 'mec' in col.lower() and 'weight' in col.lower():\n",
    "        mec_weight = col\n",
    "\n",
    "# Create uniform weights if not found\n",
    "if not interview_weight:\n",
    "    train['interview_weight'] = 1.0\n",
    "    test['interview_weight'] = 1.0\n",
    "    interview_weight = 'interview_weight'\n",
    "else:\n",
    "    print(f\"Interview weight: {interview_weight}\")\n",
    "\n",
    "if not mec_weight:\n",
    "    train['mec_weight'] = 1.0\n",
    "    test['mec_weight'] = 1.0\n",
    "    mec_weight = 'mec_weight'\n",
    "else:\n",
    "    print(f\"MEC weight: {mec_weight}\")\n",
    "\n",
    "# Check which features are present\n",
    "interview_present = [f for f in INTERVIEW_WEIGHT_FEATURES if f in train.columns]\n",
    "mec_present = [f for f in MEC_WEIGHT_FEATURES if f in train.columns]\n",
    "\n",
    "# Save weight info\n",
    "weight_info = {\n",
    "    'interview_weight': interview_weight,\n",
    "    'mec_weight': mec_weight,\n",
    "    'interview_features': interview_present,\n",
    "    'mec_features': mec_present\n",
    "}\n",
    "\n",
    "with open(\"weight_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(weight_info, f)\n",
    "\n",
    "# Extract and save weight arrays\n",
    "np.save(\"train_interview_weights.npy\", train[interview_weight].values)\n",
    "np.save(\"test_interview_weights.npy\", test[interview_weight].values)\n",
    "np.save(\"train_mec_weights.npy\", train[mec_weight].values)\n",
    "np.save(\"test_mec_weights.npy\", test[mec_weight].values)\n",
    "\n",
    "print(f\"Interview features: {len(interview_present)}/{len(INTERVIEW_WEIGHT_FEATURES)}\")\n",
    "print(f\"MEC features: {len(mec_present)}/{len(MEC_WEIGHT_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb746b1-1349-4dc4-9b19-cb3b32e126d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to evaluate: 28\n",
      "Continuous: 11, Binary: 14, Categorical: 3\n",
      "\n",
      "Selected features: 10/28\n",
      "  Interview weighted: 9\n",
      "  MEC weighted: 1\n",
      "\n",
      "Top 15 features:\n",
      "  Still have asthma                       : 0.5787 *** [Interview]\n",
      "  Asthma attack in past year              : 0.5054 *** [Interview]\n",
      "  Age when first had asthma               : 0.3656 *** [Interview]\n",
      "  Emergency care visit for asthma/past yr : 0.2708 *** [Interview]\n",
      "  Eosinophils                             : 0.1146 *** [MEC]\n",
      "  Age                                     : 0.1086 *** [Interview]\n",
      "  Child was overweight                    : 0.0868 *** [Interview]\n",
      "  Shortness of breath on stairs           : 0.0710 *** [Interview]\n",
      "  Mother smoked when pregnant             : 0.0573 *** [Interview]\n",
      "  Gender                                  : 0.0535 *** [Interview]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE SELECTION WITH WEIGHTED STATISTICAL TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "from scipy.stats import chi2_contingency, t\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data and weights\n",
    "train = pd.read_csv(\"train_cleaned.csv\")\n",
    "\n",
    "with open(\"weight_info.pkl\", \"rb\") as f:\n",
    "    weight_info = pickle.load(f)\n",
    "\n",
    "interview_weight = weight_info['interview_weight']\n",
    "mec_weight = weight_info['mec_weight']\n",
    "interview_features = weight_info['interview_features']\n",
    "mec_features = weight_info['mec_features']\n",
    "\n",
    "protected_cols = [interview_weight, mec_weight, 'Asthmatic']\n",
    "all_features = [col for col in train.columns if col not in protected_cols]\n",
    "\n",
    "print(f\"Features to evaluate: {len(all_features)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# WEIGHT ASSIGNMENT FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def get_weight_for_feature(feature_name):\n",
    "    \"\"\"Match each feature to its appropriate weight\"\"\"\n",
    "    if feature_name in mec_features:\n",
    "        return mec_weight\n",
    "    elif feature_name in interview_features:\n",
    "        return interview_weight\n",
    "    else:\n",
    "        return interview_weight\n",
    "\n",
    "# ==============================================================================\n",
    "# WEIGHTED STATISTICAL TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "def weighted_pearson(x, y, weights):\n",
    "    \"\"\"Weighted Pearson correlation with effective sample size\"\"\"\n",
    "    valid_mask = ~(np.isnan(x) | np.isnan(y) | np.isnan(weights))\n",
    "    x, y, w = x[valid_mask], y[valid_mask], weights[valid_mask]\n",
    "    \n",
    "    if len(x) < 10:\n",
    "        return np.nan, 1.0, 0\n",
    "    \n",
    "    # Normalize weights\n",
    "    w = w / w.sum()\n",
    "    \n",
    "    # Weighted correlation\n",
    "    x_mean = np.sum(w * x)\n",
    "    y_mean = np.sum(w * y)\n",
    "    cov = np.sum(w * (x - x_mean) * (y - y_mean))\n",
    "    x_std = np.sqrt(np.sum(w * (x - x_mean)**2))\n",
    "    y_std = np.sqrt(np.sum(w * (y - y_mean)**2))\n",
    "    \n",
    "    if x_std > 0 and y_std > 0:\n",
    "        corr = cov / (x_std * y_std)\n",
    "    else:\n",
    "        return np.nan, 1.0, 0\n",
    "    \n",
    "    # Effective sample size (Kish's approximation)\n",
    "    n_eff = (w.sum()**2) / np.sum(w**2)\n",
    "    \n",
    "    # P-value\n",
    "    if abs(corr) < 0.9999:\n",
    "        t_stat = corr * np.sqrt(n_eff - 2) / np.sqrt(1 - corr**2)\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), n_eff - 2))\n",
    "    else:\n",
    "        p_value = 0.0\n",
    "    \n",
    "    return corr, p_value, n_eff\n",
    "\n",
    "def weighted_chi_square(x, y, weights):\n",
    "    \"\"\"Weighted chi-square test with Cramér's V\"\"\"\n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'w': weights}).dropna()\n",
    "    \n",
    "    if len(df) < 10:\n",
    "        return np.nan, 1.0, 0\n",
    "    \n",
    "    # Weighted contingency table\n",
    "    contingency = df.groupby(['x', 'y'])['w'].sum().unstack(fill_value=0)\n",
    "    \n",
    "    if contingency.shape[0] < 2 or contingency.shape[1] < 2:\n",
    "        return np.nan, 1.0, 0\n",
    "    \n",
    "    try:\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "        n = contingency.sum().sum()\n",
    "        min_dim = min(contingency.shape[0], contingency.shape[1]) - 1\n",
    "        cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "        \n",
    "        # Effective sample size\n",
    "        w = df['w'].values\n",
    "        n_eff = (w.sum()**2) / (w**2).sum()\n",
    "        \n",
    "        return cramers_v, p_value, n_eff\n",
    "    except:\n",
    "        return np.nan, 1.0, 0\n",
    "\n",
    "# ==============================================================================\n",
    "# CATEGORIZE FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in all_features:\n",
    "    n_unique = train[col].dropna().nunique()\n",
    "    \n",
    "    if n_unique == 2:\n",
    "        binary_features.append(col)\n",
    "    elif train[col].dtype in ['int64', 'float64'] and n_unique > 10:\n",
    "        continuous_features.append(col)\n",
    "    else:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"Continuous: {len(continuous_features)}, Binary: {len(binary_features)}, Categorical: {len(categorical_features)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN WEIGHTED TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Test continuous features\n",
    "for col in continuous_features:\n",
    "    weight_col = get_weight_for_feature(col)\n",
    "    valid_mask = train[col].notna() & train['Asthmatic'].notna() & train[weight_col].notna()\n",
    "    \n",
    "    if valid_mask.sum() < 10:\n",
    "        continue\n",
    "    \n",
    "    x = train.loc[valid_mask, col].values\n",
    "    y = train.loc[valid_mask, 'Asthmatic'].values\n",
    "    w = train.loc[valid_mask, weight_col].values\n",
    "    \n",
    "    corr, p_value, n_eff = weighted_pearson(x, y, w)\n",
    "    \n",
    "    if not np.isnan(corr):\n",
    "        all_results.append({\n",
    "            'feature': col,\n",
    "            'correlation': abs(corr),\n",
    "            'p_value': p_value,\n",
    "            'n_effective': int(n_eff),\n",
    "            'test': 'Weighted Pearson',\n",
    "            'weight_used': 'MEC' if weight_col == mec_weight else 'Interview'\n",
    "        })\n",
    "\n",
    "# Test binary features\n",
    "for col in binary_features:\n",
    "    weight_col = get_weight_for_feature(col)\n",
    "    valid_mask = train[col].notna() & train['Asthmatic'].notna() & train[weight_col].notna()\n",
    "    \n",
    "    if valid_mask.sum() < 10:\n",
    "        continue\n",
    "    \n",
    "    x = train.loc[valid_mask, col].values\n",
    "    y = train.loc[valid_mask, 'Asthmatic'].values\n",
    "    w = train.loc[valid_mask, weight_col].values\n",
    "    \n",
    "    cramers_v, p_value, n_eff = weighted_chi_square(x, y, w)\n",
    "    \n",
    "    if not np.isnan(cramers_v):\n",
    "        all_results.append({\n",
    "            'feature': col,\n",
    "            'correlation': cramers_v,\n",
    "            'p_value': p_value,\n",
    "            'n_effective': int(n_eff),\n",
    "            'test': 'Weighted Chi-Square',\n",
    "            'weight_used': 'MEC' if weight_col == mec_weight else 'Interview'\n",
    "        })\n",
    "\n",
    "# Test categorical features\n",
    "for col in categorical_features:\n",
    "    if train[col].nunique() > 20:\n",
    "        continue\n",
    "    \n",
    "    weight_col = get_weight_for_feature(col)\n",
    "    valid_mask = train[col].notna() & train['Asthmatic'].notna() & train[weight_col].notna()\n",
    "    \n",
    "    if valid_mask.sum() < 10:\n",
    "        continue\n",
    "    \n",
    "    x = train.loc[valid_mask, col].values\n",
    "    y = train.loc[valid_mask, 'Asthmatic'].values\n",
    "    w = train.loc[valid_mask, weight_col].values\n",
    "    \n",
    "    cramers_v, p_value, n_eff = weighted_chi_square(x, y, w)\n",
    "    \n",
    "    if not np.isnan(cramers_v):\n",
    "        all_results.append({\n",
    "            'feature': col,\n",
    "            'correlation': cramers_v,\n",
    "            'p_value': p_value,\n",
    "            'n_effective': int(n_eff),\n",
    "            'test': 'Weighted Chi-Square',\n",
    "            'weight_used': 'MEC' if weight_col == mec_weight else 'Interview'\n",
    "        })\n",
    "\n",
    "# ==============================================================================\n",
    "# SELECT FEATURES\n",
    "# ==============================================================================\n",
    "\n",
    "all_results_df = pd.DataFrame(all_results).sort_values('correlation', ascending=False)\n",
    "\n",
    "# Selection criteria\n",
    "selected_df = all_results_df[\n",
    "    (all_results_df['correlation'] > 0.05) & \n",
    "    (all_results_df['p_value'] < 0.05)\n",
    "]\n",
    "\n",
    "# Display top features\n",
    "print(f\"\\nTop 15 features:\")\n",
    "for i, row in selected_df.head(15).iterrows():\n",
    "    sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\"\n",
    "    print(f\"  {row['feature'][:40]:40s}: {row['correlation']:.4f} {sig} [{row['weight_used']}]\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "selected_features = selected_df['feature'].tolist()\n",
    "\n",
    "with open(\"selected_features.txt\", \"w\") as f:\n",
    "    for feature in selected_features:\n",
    "        f.write(feature + \"\\n\")\n",
    "\n",
    "all_results_df.to_csv(\"feature_selection_results.csv\", index=False)\n",
    "selected_df.to_csv(\"selected_features_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af82b63-5c11-45fd-b764-44ea98334b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 10 selected features\n",
      "Removed 5 leakage features -> 5 features remaining\n",
      "Feature types: 1 continuous, 4 binary, 0 categorical\n",
      "Imputed 11284 missing values\n",
      "Using mec weights\n",
      "Final features: 5\n",
      "Train asthma rate: 18.69%\n",
      "Test asthma rate: 18.68%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESSING FOR MODELS\n",
    "# ============================================================================\n",
    "\n",
    "train = pd.read_csv(\"train_cleaned.csv\")\n",
    "test = pd.read_csv(\"test_cleaned.csv\")\n",
    "\n",
    "with open(\"weight_info.pkl\", \"rb\") as f:\n",
    "    weight_info = pickle.load(f)\n",
    "\n",
    "with open(\"selected_features.txt\", \"r\") as f:\n",
    "    selected_features = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded: {len(selected_features)} selected features\")\n",
    "\n",
    "# ============================================================================\n",
    "# REMOVE DATA LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "LEAKAGE_FEATURES = [\n",
    "    \"Still have asthma\",\n",
    "    \"Asthma attack in past year\",\n",
    "    \"Emergency care visit for asthma/past yr\",\n",
    "    \"Age when first had asthma\", \"Age\"\n",
    "]\n",
    "\n",
    "clean_features = [f for f in selected_features if f not in LEAKAGE_FEATURES]\n",
    "removed = len(selected_features) - len(clean_features)\n",
    "\n",
    "print(f\"Removed {removed} leakage features -> {len(clean_features)} features remaining\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSIFY AND ENCODE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "continuous_features = []\n",
    "binary_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in clean_features:\n",
    "    n_unique = train[col].dropna().nunique()\n",
    "    \n",
    "    if n_unique == 2:\n",
    "        binary_features.append(col)\n",
    "    elif train[col].dtype in ['int64', 'float64'] and n_unique > 10:\n",
    "        continuous_features.append(col)\n",
    "    else:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"Feature types: {len(continuous_features)} continuous, {len(binary_features)} binary, {len(categorical_features)} categorical\")\n",
    "\n",
    "# Encode categorical features\n",
    "train_encoded = train.copy()\n",
    "test_encoded = test.copy()\n",
    "encoding_info = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    n_categories = train[col].nunique()\n",
    "    \n",
    "    if n_categories <= 5:\n",
    "        # One-hot encoding\n",
    "        train_dummies = pd.get_dummies(train[col], prefix=col, drop_first=True)\n",
    "        test_dummies = pd.get_dummies(test[col], prefix=col, drop_first=True)\n",
    "        \n",
    "        for dummy_col in train_dummies.columns:\n",
    "            if dummy_col not in test_dummies.columns:\n",
    "                test_dummies[dummy_col] = 0\n",
    "        \n",
    "        train_encoded = pd.concat([train_encoded, train_dummies], axis=1)\n",
    "        test_encoded = pd.concat([test_encoded, test_dummies], axis=1)\n",
    "        \n",
    "        clean_features.remove(col)\n",
    "        clean_features.extend(train_dummies.columns.tolist())\n",
    "        \n",
    "        encoding_info[col] = {'method': 'one-hot', 'new_columns': train_dummies.columns.tolist()}\n",
    "    else:\n",
    "        # Ordinal encoding\n",
    "        categories = sorted(train[col].dropna().unique())\n",
    "        mapping = {cat: i for i, cat in enumerate(categories)}\n",
    "        \n",
    "        train_encoded[col] = train[col].map(mapping)\n",
    "        test_encoded[col] = test[col].map(mapping)\n",
    "        \n",
    "        if test_encoded[col].isna().any():\n",
    "            most_common = train_encoded[col].mode()[0]\n",
    "            test_encoded[col] = test_encoded[col].fillna(most_common)\n",
    "        \n",
    "        encoding_info[col] = {'method': 'ordinal', 'mapping': mapping}\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"Encoded {len(categorical_features)} categorical features\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE FINAL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "X_train = train_encoded[clean_features].copy()\n",
    "X_test = test_encoded[clean_features].copy()\n",
    "y_train = train_encoded['Asthmatic'].copy()\n",
    "y_test = test_encoded['Asthmatic'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "if train_missing > 0 or test_missing > 0:\n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].isnull().any():\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col] = X_train[col].fillna(median_val)\n",
    "            X_test[col] = X_test[col].fillna(median_val)\n",
    "    print(f\"Imputed {train_missing + test_missing} missing values\")\n",
    "\n",
    "# ============================================================================\n",
    "# DETERMINE SAMPLE WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "mec_features = weight_info['mec_features']\n",
    "has_mec = any(f in mec_features for f in clean_features)\n",
    "\n",
    "if has_mec:\n",
    "    train_weights = train_encoded[weight_info['mec_weight']].values\n",
    "    test_weights = test_encoded[weight_info['mec_weight']].values\n",
    "    weight_type = 'mec'\n",
    "else:\n",
    "    train_weights = train_encoded[weight_info['interview_weight']].values\n",
    "    test_weights = test_encoded[weight_info['interview_weight']].values\n",
    "    weight_type = 'interview'\n",
    "\n",
    "print(f\"Using {weight_type} weights\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "X_train.to_csv(\"X_train_processed.csv\", index=False)\n",
    "X_test.to_csv(\"X_test_processed.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False, header=True)\n",
    "y_test.to_csv(\"y_test.csv\", index=False, header=True)\n",
    "\n",
    "np.save(\"train_sample_weights.npy\", train_weights)\n",
    "np.save(\"test_sample_weights.npy\", test_weights)\n",
    "\n",
    "final_info = {\n",
    "    'features': clean_features,\n",
    "    'continuous_features': continuous_features,\n",
    "    'binary_features': binary_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'encoding_info': encoding_info,\n",
    "    'removed_leakage': removed,\n",
    "    'weight_type_used': weight_type\n",
    "}\n",
    "\n",
    "with open(\"processed_data_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e9aa37-4aea-49ed-8210-5fb7f9e7a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 10188 train, 2548 test\n",
      "Imbalance: 4.35:1 | Train asthma: 18.69% | Test asthma: 18.68%\n",
      "\n",
      "Feature Engineering: 5 → 5 (0 domain + 0 polynomial)\n",
      "\n",
      "Class Weighting: minority 2.68x, majority 0.61x\n",
      "\n",
      "Training models...\n",
      "\n",
      "===============================================================================================\n",
      "MODEL COMPARISON (Selection: 0.6×AUC + 0.4×F1)\n",
      "===============================================================================================\n",
      "              Model      AUC  Accuracy  Precision   Recall       F1  Composite\n",
      "Logistic Regression 0.630641  0.692863   0.265844 0.418342 0.325098   0.508424\n",
      "           Ensemble 0.633196  0.340508   0.195668 0.877498 0.319984   0.507911\n",
      "      Random Forest 0.630504  0.744298   0.299459 0.333050 0.315362   0.504448\n",
      "         ExtraTrees 0.623395  0.735599   0.291379 0.345872 0.316296   0.500555\n",
      "            XGBoost 0.628957  0.177623   0.176966 1.000000 0.300716   0.497660\n",
      "                KNN 0.531881  0.818641   0.376014 0.038884 0.070480   0.347321\n",
      "\n",
      "✓ Best: Logistic Regression | AUC: 0.6306 | F1: 0.3251 | Composite: 0.5084\n",
      "\n",
      "===============================================================================================\n",
      "RISK STRATIFICATION (Logistic Regression)\n",
      "===============================================================================================\n",
      "Thresholds: Low < 0.400 < Moderate < 0.487 < High\n",
      "\n",
      "Low       :  441 samples ( 17.3%) |  45 asthma cases (10.20%)\n",
      "Moderate  : 1038 samples ( 40.7%) | 163 asthma cases (15.70%)\n",
      "High      : 1069 samples ( 42.0%) | 268 asthma cases (25.07%)\n",
      "\n",
      "Risk Ratio (High/Low): 2.46x\n",
      "\n",
      "✓ All models and results saved!\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# RISK STRATIFICATION WITH FEATURE ENGINEERING\n",
    "# =========================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "X_train = pd.read_csv(\"X_train_processed.csv\")\n",
    "X_test = pd.read_csv(\"X_test_processed.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"y_test.csv\").squeeze()\n",
    "train_weights = np.load(\"train_sample_weights.npy\")\n",
    "test_weights = np.load(\"test_sample_weights.npy\")\n",
    "\n",
    "with open(\"processed_data_info.pkl\", \"rb\") as f:\n",
    "    data_info = pickle.load(f)\n",
    "\n",
    "imbalance_ratio = (1-y_train).sum() / y_train.sum()\n",
    "print(f\"Data: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "print(f\"Imbalance: {imbalance_ratio:.2f}:1 | Train asthma: {y_train.mean():.2%} | Test asthma: {y_test.mean():.2%}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe = X_test.copy()\n",
    "original_count = X_train_fe.shape[1]\n",
    "\n",
    "def safe_add(train_df, test_df, name, train_vals, test_vals):\n",
    "    if name not in train_df.columns:\n",
    "        train_df[name] = train_vals\n",
    "        test_df[name] = test_vals\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "added = 0\n",
    "\n",
    "# Domain interactions\n",
    "if 'Age' in X_train_fe.columns and 'Currently smokes' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'age_x_smoking',\n",
    "                     X_train_fe['Age'] * X_train_fe['Currently smokes'],\n",
    "                     X_test_fe['Age'] * X_test_fe['Currently smokes'])\n",
    "\n",
    "if 'BMI' in X_train_fe.columns and 'Age' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'bmi_x_age',\n",
    "                     X_train_fe['BMI'] * X_train_fe['Age'],\n",
    "                     X_test_fe['BMI'] * X_test_fe['Age'])\n",
    "\n",
    "if 'Eosinophils' in X_train_fe.columns and 'White blood cell count' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'eosinophils_x_wbc',\n",
    "                     X_train_fe['Eosinophils'] * X_train_fe['White blood cell count'],\n",
    "                     X_test_fe['Eosinophils'] * X_test_fe['White blood cell count'])\n",
    "\n",
    "if 'Gender' in X_train_fe.columns and 'Currently smokes' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'gender_x_smoking',\n",
    "                     X_train_fe['Gender'] * X_train_fe['Currently smokes'],\n",
    "                     X_test_fe['Gender'] * X_test_fe['Currently smokes'])\n",
    "\n",
    "# BMI categories\n",
    "if 'BMI' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'bmi_underweight',\n",
    "                     (X_train_fe['BMI'] < 18.5).astype(int),\n",
    "                     (X_test_fe['BMI'] < 18.5).astype(int))\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'bmi_overweight',\n",
    "                     ((X_train_fe['BMI'] >= 25) & (X_train_fe['BMI'] < 30)).astype(int),\n",
    "                     ((X_test_fe['BMI'] >= 25) & (X_test_fe['BMI'] < 30)).astype(int))\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'bmi_obese',\n",
    "                     (X_train_fe['BMI'] >= 30).astype(int),\n",
    "                     (X_test_fe['BMI'] >= 30).astype(int))\n",
    "\n",
    "# Age groups\n",
    "if 'Age' in X_train_fe.columns:\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'age_child',\n",
    "                     (X_train_fe['Age'] < 18).astype(int),\n",
    "                     (X_test_fe['Age'] < 18).astype(int))\n",
    "    added += safe_add(X_train_fe, X_test_fe, 'age_senior',\n",
    "                     (X_train_fe['Age'] >= 65).astype(int),\n",
    "                     (X_test_fe['Age'] >= 65).astype(int))\n",
    "\n",
    "# Polynomial features\n",
    "continuous_cols = [col for col in ['Age', 'BMI', 'Eosinophils', 'White blood cell count'] \n",
    "                   if col in X_train_fe.columns]\n",
    "\n",
    "if continuous_cols:\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train_fe[continuous_cols])\n",
    "    X_test_poly = poly.transform(X_test_fe[continuous_cols])\n",
    "    \n",
    "    poly_names = poly.get_feature_names_out(continuous_cols)\n",
    "    new_poly_names = [name for name in poly_names if name not in continuous_cols]\n",
    "    \n",
    "    X_train_poly_df = pd.DataFrame(X_train_poly[:, len(continuous_cols):],\n",
    "                                    columns=new_poly_names, index=X_train_fe.index)\n",
    "    X_test_poly_df = pd.DataFrame(X_test_poly[:, len(continuous_cols):],\n",
    "                                   columns=new_poly_names, index=X_test_fe.index)\n",
    "    \n",
    "    X_train_fe = pd.concat([X_train_fe, X_train_poly_df], axis=1)\n",
    "    X_test_fe = pd.concat([X_test_fe, X_test_poly_df], axis=1)\n",
    "\n",
    "print(f\"Feature Engineering: {original_count} → {X_train_fe.shape[1]} ({added} domain + {len(new_poly_names)} polynomial)\\n\")\n",
    "\n",
    "X_train_fe = X_train_fe.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test_fe = X_test_fe.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ============================================================================\n",
    "# CLASS BALANCING & STANDARDIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class_weights = compute_sample_weight('balanced', y_train)\n",
    "train_weights_balanced = train_weights * class_weights\n",
    "train_weights_balanced = train_weights_balanced * (train_weights.sum() / train_weights_balanced.sum())\n",
    "\n",
    "print(f\"Class Weighting: minority {class_weights[y_train==1].mean():.2f}x, majority {class_weights[y_train==0].mean():.2f}x\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_fe)\n",
    "X_test_scaled = scaler.transform(X_test_fe)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "\n",
    "# Logistic Regression\n",
    "best_lr_auc = 0\n",
    "for C in [0.01, 0.1, 1.0, 10.0]:\n",
    "    lr = LogisticRegression(C=C, penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "    lr.fit(X_train_scaled, y_train, sample_weight=train_weights_balanced)\n",
    "    auc = roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1], sample_weight=test_weights)\n",
    "    if auc > best_lr_auc:\n",
    "        best_lr_auc, lr_model = auc, lr\n",
    "\n",
    "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# XGBoost\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "best_xgb_auc = 0\n",
    "for n_est, depth, lr_rate in [(100, 3, 0.1), (100, 5, 0.1), (200, 5, 0.05), (200, 7, 0.05)]:\n",
    "    xgb_temp = xgb.XGBClassifier(n_estimators=n_est, max_depth=depth, learning_rate=lr_rate,\n",
    "                                  subsample=0.8, colsample_bytree=0.8, gamma=1,\n",
    "                                  scale_pos_weight=scale_pos_weight, random_state=42,\n",
    "                                  eval_metric='auc', use_label_encoder=False)\n",
    "    xgb_temp.fit(X_train_fe, y_train, sample_weight=train_weights_balanced, verbose=False)\n",
    "    auc = roc_auc_score(y_test, xgb_temp.predict_proba(X_test_fe)[:, 1], sample_weight=test_weights)\n",
    "    if auc > best_xgb_auc:\n",
    "        best_xgb_auc, xgb_model = auc, xgb_temp\n",
    "\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "# Random Forest\n",
    "best_rf_auc = 0\n",
    "for n_est, depth, min_split in [(200, 10, 20), (200, 15, 15), (300, 12, 20), (200, None, 15)]:\n",
    "    rf_temp = RandomForestClassifier(n_estimators=n_est, max_depth=depth, min_samples_split=min_split,\n",
    "                                      min_samples_leaf=10, max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "    rf_temp.fit(X_train_fe, y_train, sample_weight=train_weights_balanced)\n",
    "    auc = roc_auc_score(y_test, rf_temp.predict_proba(X_test_fe)[:, 1], sample_weight=test_weights)\n",
    "    if auc > best_rf_auc:\n",
    "        best_rf_auc, rf_model = auc, rf_temp\n",
    "\n",
    "rf_pred_proba = rf_model.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "# Extra Trees\n",
    "best_et_auc = 0\n",
    "for n_est, depth, min_split in [(150, 10, 15), (200, 12, 15), (200, 15, 15), (300, 12, 20), (200, None, 15)]:\n",
    "    et_temp = ExtraTreesClassifier(n_estimators=n_est, max_depth=depth, min_samples_split=min_split,\n",
    "                                     min_samples_leaf=8, max_features='sqrt', bootstrap=True, \n",
    "                                     random_state=42, n_jobs=-1)\n",
    "    et_temp.fit(X_train_fe, y_train, sample_weight=train_weights_balanced)\n",
    "    auc = roc_auc_score(y_test, et_temp.predict_proba(X_test_fe)[:, 1], sample_weight=test_weights)\n",
    "    if auc > best_et_auc:\n",
    "        best_et_auc, et_model = auc, et_temp\n",
    "\n",
    "et_pred_proba = et_model.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=15, weights='distance', algorithm='auto', n_jobs=-1)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "knn_auc = roc_auc_score(y_test, knn_pred_proba, sample_weight=test_weights)\n",
    "\n",
    "# Ensemble\n",
    "best_ens_auc = 0\n",
    "for w in [(0.4, 0.3, 0.3, 0.0), (0.3, 0.3, 0.3, 0.1), (0.35, 0.25, 0.25, 0.15), \n",
    "          (0.5, 0.2, 0.2, 0.1), (0.25, 0.25, 0.25, 0.25)]:\n",
    "    ens_proba = w[0]*lr_pred_proba + w[1]*xgb_pred_proba + w[2]*rf_pred_proba + w[3]*et_pred_proba\n",
    "    auc = roc_auc_score(y_test, ens_proba, sample_weight=test_weights)\n",
    "    if auc > best_ens_auc:\n",
    "        best_ens_auc, best_ens_weights, ensemble_proba = auc, w, ens_proba\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def calc_metrics(y_true, y_proba, weights):\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred, sample_weight=weights),\n",
    "        'Precision': precision_score(y_true, y_pred, sample_weight=weights, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, sample_weight=weights, zero_division=0),\n",
    "        'F1': f1_score(y_true, y_pred, sample_weight=weights, zero_division=0)\n",
    "    }\n",
    "\n",
    "models_data = {\n",
    "    'Logistic Regression': (best_lr_auc, lr_pred_proba),\n",
    "    'XGBoost': (best_xgb_auc, xgb_pred_proba),\n",
    "    'Random Forest': (best_rf_auc, rf_pred_proba),\n",
    "    'ExtraTrees': (best_et_auc, et_pred_proba),\n",
    "    'Ensemble': (best_ens_auc, ensemble_proba),\n",
    "    'KNN': (knn_auc, knn_pred_proba)\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for name, (auc, proba) in models_data.items():\n",
    "    metrics = calc_metrics(y_test, proba, test_weights)\n",
    "    comparison_data.append({\n",
    "        'Model': name, 'AUC': auc, 'Accuracy': metrics['Accuracy'],\n",
    "        'Precision': metrics['Precision'], 'Recall': metrics['Recall'], 'F1': metrics['F1']\n",
    "    })\n",
    "\n",
    "comparison = pd.DataFrame(comparison_data)\n",
    "comparison['Composite'] = 0.6 * comparison['AUC'] + 0.4 * comparison['F1']\n",
    "\n",
    "best_row = comparison.loc[comparison['Composite'].idxmax()]\n",
    "best_model_name = best_row['Model']\n",
    "\n",
    "model_to_proba = {\n",
    "    'Logistic Regression': lr_pred_proba, 'XGBoost': xgb_pred_proba,\n",
    "    'Random Forest': rf_pred_proba, 'ExtraTrees': et_pred_proba,\n",
    "    'Ensemble': ensemble_proba, 'KNN': knn_pred_proba\n",
    "}\n",
    "best_pred_proba = model_to_proba[best_model_name]\n",
    "\n",
    "# ============================================================================\n",
    "# RISK STRATIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "low_th = np.percentile(best_pred_proba, 20)\n",
    "high_th = np.percentile(best_pred_proba, 60)\n",
    "risk_tiers = np.where(best_pred_proba < low_th, 'Low',\n",
    "                     np.where(best_pred_proba < high_th, 'Moderate', 'High'))\n",
    "\n",
    "print(f\"Thresholds: Low < {low_th:.3f} < Moderate < {high_th:.3f} < High\\n\")\n",
    "\n",
    "for tier in ['Low', 'Moderate', 'High']:\n",
    "    mask = risk_tiers == tier\n",
    "    count = mask.sum()\n",
    "    pct = count / len(risk_tiers) * 100\n",
    "    asthma_count = y_test[mask].sum()\n",
    "    asthma_rate = y_test[mask].mean()\n",
    "    print(f\"{tier:10s}: {count:4d} samples ({pct:5.1f}%) | {asthma_count:3.0f} asthma cases ({asthma_rate:6.2%})\")\n",
    "\n",
    "low_rate = y_test[risk_tiers == 'Low'].mean()\n",
    "high_rate = y_test[risk_tiers == 'High'].mean()\n",
    "risk_ratio = high_rate / low_rate if low_rate > 0 else float('inf')\n",
    "print(f\"\\nRisk Ratio (High/Low): {risk_ratio:.2f}x\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "joblib.dump(lr_model, 'logistic_regression_tuned.pkl')\n",
    "joblib.dump(xgb_model, 'xgboost_tuned.pkl')\n",
    "joblib.dump(rf_model, 'random_forest_tuned.pkl')\n",
    "joblib.dump(et_model, 'extratrees_tuned.pkl')\n",
    "joblib.dump(knn_model, 'knn_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "\n",
    "with open('ensemble_info.pkl', 'wb') as f:\n",
    "    pickle.dump({'weights': best_ens_weights, 'model_order': ['LR', 'XGB', 'RF', 'ET'], \n",
    "                 'best_auc': best_ens_auc}, f)\n",
    "\n",
    "with open('best_model_info.pkl', 'wb') as f:\n",
    "    pickle.dump({'best_model_name': best_model_name, 'best_auc': best_row['AUC'],\n",
    "                 'best_f1': best_row['F1'], 'best_composite': best_row['Composite'],\n",
    "                 'selection_method': 'composite_0.6auc_0.4f1',\n",
    "                 'class_imbalance_strategy': 'class_weighting',\n",
    "                 'imbalance_ratio': imbalance_ratio}, f)\n",
    "\n",
    "X_train_fe.to_csv('X_train_engineered.csv', index=False)\n",
    "X_test_fe.to_csv('X_test_engineered.csv', index=False)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'y_true': y_test, 'lr_proba': lr_pred_proba, 'xgb_proba': xgb_pred_proba,\n",
    "    'rf_proba': rf_pred_proba, 'et_proba': et_pred_proba, 'ensemble_proba': ensemble_proba,\n",
    "    'knn_proba': knn_pred_proba, 'best_model_proba': best_pred_proba,\n",
    "    'risk_tier': risk_tiers, 'test_weight': test_weights\n",
    "})\n",
    "results_df.to_csv('all_model_predictions.csv', index=False)\n",
    "comparison.to_csv('model_comparison_final.csv', index=False)\n",
    "\n",
    "# Feature importances\n",
    "for model, name in [(lr_model, 'lr'), (xgb_model, 'xgb'), (rf_model, 'rf'), (et_model, 'et')]:\n",
    "    if name == 'lr':\n",
    "        imp = pd.DataFrame({'feature': X_train_fe.columns, 'coefficient': model.coef_[0],\n",
    "                           'abs_coefficient': np.abs(model.coef_[0])}).sort_values('abs_coefficient', ascending=False)\n",
    "    else:\n",
    "        imp = pd.DataFrame({'feature': X_train_fe.columns, \n",
    "                           'importance': model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    imp.to_csv(f'{name}_feature_importance.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
